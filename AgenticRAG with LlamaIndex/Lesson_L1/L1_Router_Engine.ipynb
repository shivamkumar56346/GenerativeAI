{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7c4c87",
   "metadata": {},
   "source": [
    "# Lesson 1: Router Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877df988",
   "metadata": {},
   "source": [
    "Welcome to Lesson 1.\n",
    "\n",
    "To access the `requirements.txt` file, the data/pdf file required for this lesson and the `helper` and `utils` modules, please go to the `File` menu and select`Open...`.\n",
    "\n",
    "I hope you enjoy this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c97c9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5f4f4a-5890-451c-8869-24606ef9f396",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c00bbcf",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API KEY: 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\n"
     ]
    }
   ],
   "source": [
    "print(\"API KEY:\",OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffc9b4f4-64d4-4266-9889-54db90e00ee9",
   "metadata": {
    "height": 64,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fca250",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae2a8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "To download this paper, below is the needed code:\n",
    "\n",
    "#!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O metagpt.pdf\n",
    "\n",
    "**Note**: The pdf file is included with this lesson. To access it, go to the `File` menu and select`Open...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7f012d-dcd3-4881-a568-72dd27d79159",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     c:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning\n",
      "[nltk_data]     AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-\n",
      "[nltk_data]     packages\\llama_index\\core\\_static/nltk_cache...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=[\"metagpt.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9efd2ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 29 documents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "429fb975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document preview: Preprint\n",
      "METAGPT: M ETA PROGRAMMING FOR A\n",
      "MULTI -AGENT COLLABORATIVE FRAMEWORK\n",
      "Sirui Hong1∗, Mingchen Zhuge2∗, Jonathan Chen1, Xiawu Zheng3, Yuheng Cheng4,\n",
      "Ceyao Zhang4, Jinlin Wang1, Zili Wang, Steve...\n"
     ]
    }
   ],
   "source": [
    "# Print first 200 characters of the first document\n",
    "print(f\"First document preview: {documents[0].get_text()[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48a301",
   "metadata": {},
   "source": [
    "## Define LLM and Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a537bc0-78ee-4dda-a43f-60fd80062df6",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37077e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 34\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total nodes: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffa28476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (1.1.0)\n",
      "Collecting openai\n",
      "  Downloading openai-1.101.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.101.0-py3-none-any.whl (810 kB)\n",
      "   ---------------------------------------- 0.0/810.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 810.8/810.8 kB 8.8 MB/s  0:00:00\n",
      "Downloading jiter-0.10.0-cp312-cp312-win_amd64.whl (206 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "\n",
      "  Attempting uninstall: openai\n",
      "\n",
      "    Found existing installation: openai 1.1.0\n",
      "\n",
      "    Uninstalling openai-1.1.0:\n",
      "\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "      Successfully uninstalled openai-1.1.0\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   -------------------- ------------------- 1/2 [openai]\n",
      "   ---------------------------------------- 2/2 [openai]\n",
      "\n",
      "Successfully installed jiter-0.10.0 openai-1.101.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f13d0123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 1.101.0\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: https://github.com/openai/openai-python\n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: Apache-2.0\n",
      "Location: C:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\n",
      "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: llama-index-legacy\n"
     ]
    }
   ],
   "source": [
    "# !pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51d55c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-core<0.11.0,>=0.10.1\n",
      "  Using cached llama_index_core-0.10.68.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.5\n",
      "  Downloading llama_index_llms_openai-0.1.31-py3-none-any.whl.metadata (650 bytes)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1) (2.0.43)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (3.12.15)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (2025.7.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (3.5)\n",
      "Requirement already satisfied: nltk!=3.9,>=3.8.1 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (2.2.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (11.3.0)\n",
      "Requirement already satisfied: pydantic<3.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (2.11.7)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (2.32.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (0.11.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (4.14.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (0.9.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.1) (1.17.3)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.40.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from llama-index-llms-openai<0.2.0,>=0.1.5) (1.101.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1) (1.20.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.2.0,>=0.1.5) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.2.0,>=0.1.5) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.2.0,>=0.1.5) (0.10.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.2.0,>=0.1.5) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.2.0,>=0.1.5) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from pydantic<3.0->llama-index-core<0.11.0,>=0.10.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from pydantic<3.0->llama-index-core<0.11.0,>=0.10.1) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from pydantic<3.0->llama-index-core<0.11.0,>=0.10.1) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.1) (0.4.6)\n",
      "Requirement already satisfied: click in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.1) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.1) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from nltk!=3.9,>=3.8.1->llama-index-core<0.11.0,>=0.10.1) (2025.7.34)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.11.0,>=0.10.1) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shiva\\onedrive\\videos\\deep learning ai\\agenticrag with llamaindex\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1) (1.17.0)\n",
      "Using cached llama_index_core-0.10.68.post1-py3-none-any.whl (1.6 MB)\n",
      "Downloading llama_index_llms_openai-0.1.31-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: llama-index-core, llama-index-llms-openai\n",
      "\n",
      "  Attempting uninstall: llama-index-core\n",
      "\n",
      "    Found existing installation: llama-index-core 0.13.3\n",
      "\n",
      "    Uninstalling llama-index-core-0.13.3:\n",
      "\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "      Successfully uninstalled llama-index-core-0.13.3\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "  Attempting uninstall: llama-index-llms-openai\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "    Found existing installation: llama-index-llms-openai 0.5.4\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "    Uninstalling llama-index-llms-openai-0.5.4:\n",
      "   ---------------------------------------- 0/2 [llama-index-core]\n",
      "   -------------------- ------------------- 1/2 [llama-index-llms-openai]\n",
      "      Successfully uninstalled llama-index-llms-openai-0.5.4\n",
      "   -------------------- ------------------- 1/2 [llama-index-llms-openai]\n",
      "   ---------------------------------------- 2/2 [llama-index-llms-openai]\n",
      "\n",
      "Successfully installed llama-index-core-0.10.68.post1 llama-index-llms-openai-0.1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llama-cloud-services 0.6.54 requires llama-index-core>=0.12.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\n",
      "llama-index 0.13.2 requires llama-index-core<0.14,>=0.13.2, but you have llama-index-core 0.10.68.post1 which is incompatible.\n",
      "llama-index 0.13.2 requires llama-index-llms-openai<0.6,>=0.5.0, but you have llama-index-llms-openai 0.1.31 which is incompatible.\n",
      "llama-index-cli 0.5.0 requires llama-index-core<0.14,>=0.13.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\n",
      "llama-index-cli 0.5.0 requires llama-index-llms-openai<0.6,>=0.5.0, but you have llama-index-llms-openai 0.1.31 which is incompatible.\n",
      "llama-index-embeddings-openai 0.5.0 requires llama-index-core<0.14,>=0.13.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\n",
      "llama-index-indices-managed-llama-cloud 0.9.2 requires llama-index-core<0.14,>=0.13.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\n",
      "llama-index-readers-file 0.5.2 requires llama-index-core<0.14,>=0.13.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\n",
      "llama-index-readers-llama-parse 0.5.0 requires llama-index-core<0.14,>=0.13.0, but you have llama-index-core 0.10.68.post1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade llama-index llama-index-core llama-index-llms-openai llama-index-embeddings-openai\n",
    "\n",
    "#!pip install \"llama-index-core<0.11.0,>=0.10.1\" \"llama-index-llms-openai<0.2.0,>=0.1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76ba0427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: llama-index\n",
      "Version: 0.13.2\n",
      "Summary: Interface between LLMs and your data\n",
      "Home-page: https://llamaindex.ai\n",
      "Author: \n",
      "Author-email: Jerry Liu <jerry@llamaindex.ai>\n",
      "License-Expression: MIT\n",
      "Location: C:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\n",
      "Requires: llama-index-cli, llama-index-core, llama-index-embeddings-openai, llama-index-indices-managed-llama-cloud, llama-index-llms-openai, llama-index-readers-file, llama-index-readers-llama-parse, nltk\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# !pip show llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de0660ee-b231-4351-b158-d8ad023e00b5",
   "metadata": {
    "height": 115,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997c7559",
   "metadata": {},
   "source": [
    "## Define Summary Index and Vector Index over the Same Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73d01b01-bc74-432a-8d92-07b9e86498b0",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 00:07:43,453 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:07:43,454 - INFO - Retrying request to /embeddings in 0.377033 seconds\n",
      "2025-08-23 00:07:44,708 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:07:44,708 - INFO - Retrying request to /embeddings in 0.799152 seconds\n",
      "2025-08-23 00:07:46,192 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:07:46,192 - INFO - Retrying request to /embeddings in 1.894643 seconds\n",
      "2025-08-23 00:07:48,750 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:07:48,750 - INFO - Retrying request to /embeddings in 3.961656 seconds\n",
      "2025-08-23 00:07:53,024 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:07:53,025 - INFO - Retrying request to /embeddings in 6.103254 seconds\n",
      "2025-08-23 00:07:59,789 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:07:59,792 - INFO - Retrying request to /embeddings in 7.592635 seconds\n",
      "2025-08-23 00:08:08,338 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:08:08,338 - INFO - Retrying request to /embeddings in 6.141417 seconds\n",
      "2025-08-23 00:08:16,089 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:08:16,091 - INFO - Retrying request to /embeddings in 6.244579 seconds\n",
      "2025-08-23 00:08:22,686 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:08:22,686 - INFO - Retrying request to /embeddings in 7.339663 seconds\n",
      "2025-08-23 00:08:30,384 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:08:30,386 - INFO - Retrying request to /embeddings in 7.592211 seconds\n",
      "2025-08-23 00:08:38,338 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:08:38,353 - WARNING - Retrying llama_index.embeddings.openai.base.OpenAIEmbedding._get_text_embeddings.<locals>._retryable_get_embeddings in 0.6037637156309301 seconds as it raised RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}.\n",
      "2025-08-23 00:08:39,236 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:08:39,236 - INFO - Retrying request to /embeddings in 0.491841 seconds\n",
      "2025-08-23 00:08:40,155 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:08:40,155 - INFO - Retrying request to /embeddings in 0.887225 seconds\n",
      "2025-08-23 00:08:41,352 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:08:41,353 - INFO - Retrying request to /embeddings in 1.996904 seconds\n",
      "2025-08-23 00:08:43,633 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:08:43,639 - INFO - Retrying request to /embeddings in 3.635705 seconds\n",
      "2025-08-23 00:08:47,902 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:08:47,903 - INFO - Retrying request to /embeddings in 6.929065 seconds\n",
      "2025-08-23 00:08:57,329 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 429 Too Many Requests\"\n",
      "2025-08-23 00:08:57,331 - INFO - Retrying request to /embeddings in 7.790950 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\openai\\_base_client.py:1027\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1026\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/embeddings'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SummaryIndex, VectorStoreIndex\n\u001b[32m      3\u001b[39m summary_index = SummaryIndex(nodes)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m vector_index = \u001b[43mVectorStoreIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:78\u001b[39m, in \u001b[36mVectorStoreIndex.__init__\u001b[39m\u001b[34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mself\u001b[39m._embed_model = (\n\u001b[32m     72\u001b[39m     resolve_embed_model(embed_model, callback_manager=callback_manager)\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m embed_model\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m embed_model_from_settings_or_context(Settings, service_context)\n\u001b[32m     75\u001b[39m )\n\u001b[32m     77\u001b[39m \u001b[38;5;28mself\u001b[39m._insert_batch_size = insert_batch_size\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\llama_index\\core\\indices\\base.py:94\u001b[39m, in \u001b[36mBaseIndex.__init__\u001b[39m\u001b[34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, service_context, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     93\u001b[39m     nodes = nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     index_struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28mself\u001b[39m._index_struct = index_struct\n\u001b[32m     98\u001b[39m \u001b[38;5;28mself\u001b[39m._storage_context.index_store.add_index_struct(\u001b[38;5;28mself\u001b[39m._index_struct)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:314\u001b[39m, in \u001b[36mVectorStoreIndex.build_index_from_nodes\u001b[39m\u001b[34m(self, nodes, **insert_kwargs)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    307\u001b[39m     node.get_content(metadata_mode=MetadataMode.EMBED) == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[32m    308\u001b[39m ):\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    310\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot build index from nodes with no content. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    311\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure all nodes have content.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    312\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:285\u001b[39m, in \u001b[36mVectorStoreIndex._build_index_from_nodes\u001b[39m\u001b[34m(self, nodes, **insert_kwargs)\u001b[39m\n\u001b[32m    283\u001b[39m     run_async_tasks(tasks)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_nodes_to_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minsert_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m index_struct\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:238\u001b[39m, in \u001b[36mVectorStoreIndex._add_nodes_to_index\u001b[39m\u001b[34m(self, index_struct, nodes, show_progress, **insert_kwargs)\u001b[39m\n\u001b[32m    235\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m nodes_batch \u001b[38;5;129;01min\u001b[39;00m iter_batch(nodes, \u001b[38;5;28mself\u001b[39m._insert_batch_size):\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     nodes_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_node_with_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m     new_ids = \u001b[38;5;28mself\u001b[39m._vector_store.add(nodes_batch, **insert_kwargs)\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._vector_store.stores_text \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._store_nodes_override:\n\u001b[32m    242\u001b[39m         \u001b[38;5;66;03m# NOTE: if the vector store doesn't store text,\u001b[39;00m\n\u001b[32m    243\u001b[39m         \u001b[38;5;66;03m# we need to add the nodes to the index struct and document store\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\llama_index\\core\\indices\\vector_store\\base.py:145\u001b[39m, in \u001b[36mVectorStoreIndex._get_node_with_embedding\u001b[39m\u001b[34m(self, nodes, show_progress)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_node_with_embedding\u001b[39m(\n\u001b[32m    134\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    135\u001b[39m     nodes: Sequence[BaseNode],\n\u001b[32m    136\u001b[39m     show_progress: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    137\u001b[39m ) -> List[BaseNode]:\n\u001b[32m    138\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[33;03m    Get tuples of id, node, and embedding.\u001b[39;00m\n\u001b[32m    140\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \n\u001b[32m    144\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     id_to_embed_map = \u001b[43membed_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     results = []\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\llama_index\\core\\indices\\utils.py:138\u001b[39m, in \u001b[36membed_nodes\u001b[39m\u001b[34m(nodes, embed_model, show_progress)\u001b[39m\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    136\u001b[39m         id_to_embed_map[node.node_id] = node.embedding\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m new_embeddings = \u001b[43membed_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_text_embedding_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts_to_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m new_id, text_embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids_to_embed, new_embeddings):\n\u001b[32m    143\u001b[39m     id_to_embed_map[new_id] = text_embedding\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:260\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    253\u001b[39m     id_=id_,\n\u001b[32m    254\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    257\u001b[39m     tags=tags,\n\u001b[32m    258\u001b[39m )\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    262\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\llama_index\\core\\base\\embeddings\\base.py:332\u001b[39m, in \u001b[36mBaseEmbedding.get_text_embedding_batch\u001b[39m\u001b[34m(self, texts, show_progress, **kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m dispatcher.event(\n\u001b[32m    324\u001b[39m     EmbeddingStartEvent(\n\u001b[32m    325\u001b[39m         model_dict=model_dict,\n\u001b[32m    326\u001b[39m     )\n\u001b[32m    327\u001b[39m )\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callback_manager.event(\n\u001b[32m    329\u001b[39m     CBEventType.EMBEDDING,\n\u001b[32m    330\u001b[39m     payload={EventPayload.SERIALIZED: \u001b[38;5;28mself\u001b[39m.to_dict()},\n\u001b[32m    331\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     result_embeddings.extend(embeddings)\n\u001b[32m    334\u001b[39m     event.on_end(\n\u001b[32m    335\u001b[39m         payload={\n\u001b[32m    336\u001b[39m             EventPayload.CHUNKS: cur_batch,\n\u001b[32m    337\u001b[39m             EventPayload.EMBEDDINGS: embeddings,\n\u001b[32m    338\u001b[39m         },\n\u001b[32m    339\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\llama_index\\embeddings\\openai\\base.py:472\u001b[39m, in \u001b[36mOpenAIEmbedding._get_text_embeddings\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_retryable_get_embeddings\u001b[39m():\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m get_embeddings(\n\u001b[32m    466\u001b[39m         client,\n\u001b[32m    467\u001b[39m         texts,\n\u001b[32m    468\u001b[39m         engine=\u001b[38;5;28mself\u001b[39m._text_engine,\n\u001b[32m    469\u001b[39m         **\u001b[38;5;28mself\u001b[39m.additional_kwargs,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_retryable_get_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\tenacity\\__init__.py:336\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    334\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    335\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\tenacity\\__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\tenacity\\__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\tenacity\\__init__.py:398\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    399\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\tenacity\\__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    480\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\llama_index\\embeddings\\openai\\base.py:465\u001b[39m, in \u001b[36mOpenAIEmbedding._get_text_embeddings.<locals>._retryable_get_embeddings\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_retryable_get_embeddings\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_text_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\llama_index\\embeddings\\openai\\base.py:172\u001b[39m, in \u001b[36mget_embeddings\u001b[39m\u001b[34m(client, list_of_text, engine, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(list_of_text) <= \u001b[32m2048\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mThe batch size should not be larger than 2048.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    170\u001b[39m list_of_text = [text.replace(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m list_of_text]\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m data = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mlist_of_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.data\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [d.embedding \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\openai\\_base_client.py:1033\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1032\u001b[39m     err.response.close()\n\u001b[32m-> \u001b[39m\u001b[32m1033\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sleep_for_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiva\\OneDrive\\Videos\\Deep Learning AI\\AgenticRAG with LlamaIndex\\venv\\Lib\\site-packages\\openai\\_base_client.py:1073\u001b[39m, in \u001b[36mSyncAPIClient._sleep_for_retry\u001b[39m\u001b[34m(self, retries_taken, max_retries, options, response)\u001b[39m\n\u001b[32m   1070\u001b[39m timeout = \u001b[38;5;28mself\u001b[39m._calculate_retry_timeout(remaining_retries, options, response.headers \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1071\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1073\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9898d3f",
   "metadata": {},
   "source": [
    "## Define Query Engines and Set Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44cd7046-c714-4920-b077-b3ded917862f",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=True,\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a1d6d75-247e-426a-8ef4-b49225c24796",
   "metadata": {
    "height": 285,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine,\n",
    "    description=(\n",
    "        \"Useful for summarization questions related to MetaGPT\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=(\n",
    "        \"Useful for retrieving specific context from the MetaGPT paper.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2c152",
   "metadata": {},
   "source": [
    "## Define Router Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00734d7c-638a-4d63-ab1f-7f5a92a65119",
   "metadata": {
    "height": 217,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe3f0a76-68a8-444d-867f-d084bb3ff112",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: Useful for summarization questions related to MetaGPT.\n",
      "\u001b[0mThe document introduces MetaGPT, a meta-programming framework that enhances multi-agent systems using Large Language Models (LLMs) by incorporating human-like Standardized Operating Procedures (SOPs). It assigns specific roles to agents, streamlines workflows, and improves task decomposition, ensuring efficient collaboration through structured outputs and a communication protocol. MetaGPT achieves state-of-the-art performance in code generation benchmarks, emphasizing role specialization, workflow management, and efficient sharing mechanisms. The framework also includes an executable feedback mechanism to iteratively enhance code quality. Additionally, the document discusses the software development process with MetaGPT, highlighting its performance in generating executable code, the impact of different prompts on outcomes, and the challenges and solutions provided by MetaGPT in addressing issues like context efficiency, hallucinations, information overload, transparency, and accountability.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3fedea0-f2a9-46bb-8aaf-287df65b8fff",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(response.source_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af8c31b3-8e22-4ad9-9825-b8de21bd03c0",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it specifically mentions retrieving specific context from the MetaGPT paper, which would likely include information on how agents share information with other agents..\n",
      "\u001b[0mAgents share information with other agents by utilizing a shared message pool where they can publish structured messages. This shared message pool allows all agents to exchange messages directly, enabling them to both publish their own messages and access messages from other agents transparently. Additionally, agents can subscribe to relevant messages based on their role profiles, allowing them to extract the information they need for their specific tasks and responsibilities.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How do agents share information with other agents?\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed060ee",
   "metadata": {},
   "source": [
    "## Let's put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8f92e0b-1c54-489b-b8dd-41ebaafb380a",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from utils import get_router_query_engine\n",
    "\n",
    "query_engine = get_router_query_engine(\"metagpt.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec1a43f3-77dc-472a-8adc-56551c00a0ff",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: Ablation study results are specific context from the MetaGPT paper, making choice 2 the most relevant..\n",
      "\u001b[0mThe ablation study results provide insights into the impact of removing certain components or features from a system or model. This analysis helps in understanding the contribution and importance of individual elements towards the overall performance or functionality of the system.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Tell me about the ablation study results?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab623ac",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfefe1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
